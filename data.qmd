---
title: Data
description: We describe the sources of our data and the cleaning process.
toc: true
draft: false
---
## Understanding the Dataset

### Where to find the Datasets
We found the data set by googling keywords such as “mental health, race, age, substance abuse” accompanied with “data set, csv, R”. This allowed us to find the codebook from the “Substance Abuse and Mental Health Services Administration”, describing a survey on substance abuse, mental health, and race from 2021. This survey was the “National Survey on Drug Use and Health”. We deemed this source reputable and we found an R file to download from: https://www.datafiles.samhsa.gov/dataset/national-survey-drug-use-and-health-2021-nsduh-2021-ds0001. 
The link to the codebook is here:  
https://www.datafiles.samhsa.gov/sites/default/files/field-uploads-protected/studies/NSDUH-2021/NSDUH-2021-datasets/NSDUH-2021-DS0001/NSDUH-2021-DS0001-info/NSDUH-2021-DS0001-info-codebook.pdf. 

![](./images/samhsaimage.jpeg)
The survey was conducted by SAMSHA - Substance Abuse and Mental Health Services Administration which is a government agency for the United States of America.

Another dataset we found that we needed was the Census data for 2020 and getting the counties with their respective population count. This was important to to plot the counties onto a map of America so to show which cities are clustered with people more and associate it with the survey of which areas are considered large, small, or non metro areas. There are many variables that are collected by the Bureau but the most important ones that are relevant to the main dataset is the county names to get each county and the population size of each county such that it can be plotted onto the map of the United States as a heatmap of population density. The link to the US Census data for 2020 is https://www.census.gov/programs-surveys/decennial-census/decade/2020/2020-census-results.html. 

![](./images/census1.png){width="75%"}


### Data Collection and Purpose
The primary purpose of this data and the NSDUH organization was to measure the frequency and relationship between substance abuse and mental health issues. The target population is the civilian and noninstitutionalized one. This survey was sent out to and filled out by individual persons in the USA, aged 12 and over, the data was collected cross-sectionally. The survey collectors warn that this data is dependent on respondent’s truthfulness and memory, and should thus be taken with a grain of salt. Also, this survey does not include data from citizens who are active-duty military members or living in institutional group shelters (homeless shelters, hospitals, prisons, etc.). They warn that this may underestimate the use of certain drugs like heroin. 

For the creation of the census, it was created by the United States Census Bureau Agency. It is done every 10 years and is a way to produce data on the American people and the economy of the United States as well. The hope in doing this is that the United States gets a better sense of how the American people are doing and where many people are located and understanding more about what kinds of decisions need to be made for the people or society. The census is conducted through electronic questionnaires to business, homes and many other places where someone may be. Sometimes they will sample people and conduct in person interviews as well but it is not possible to do it for every single person in America so relying on electronic questionnaires and other forms of filling the census are essential in the Bereau's methods of collection.

### Different data files used 
While we used the R file available to download to make the cleaned data set in our final project repository, we used the codebook to actually understand the data set R dataset file. The codebook contained information about each column and it was broken down into sections of self-administered substance abuse, imputed substance abuse, other self-administered sections, demographics, and geographic. We divided the sections and went through them, when sorting through the substance abuse sections, we stayed consistent in the types of columns we chose, those were: “age when first used”, “time since last used”, “number of days per week used in last 12 months”, and “best estimate of how many times used in last 30 days”. 

While the data set we are using from the 2021 NSDUH contains bountiful information concerning substance use, race, income, mental health, education, and age, it does not include very specific location information. The main information provided about the location of the survey respondents is stored in the variables "PDEN10" and "COUNTYP4". This  

### Cleaning for the data

This is the link to the script, [cleaning script](/scripts/load_and_clean_data.R). The Script cleans the data by keeping only the columns that have enough responses to the questions and also are highly relevant questions that can be used in understanding the data. Our strategy is too use the columns with the most value in analysis. Questions like "Have you ever smoked a cigarette" have high response rates and gives key insight into how we can associate drug/substance use with what we are trying to look into. The script filtering involved taking all the questions we deemed qualified to be included in and the rest of the 2,800 variables in this dataset to be filtered out and to keep a consistent set of questions for the analysis use. Such selecting looks like:

```
drug_health_data_clean_new <- PUF2021_100622 |>
  select(QUESTID2, filedate,
         cigever, cigage, mjever, mjage, alcever, cocever, cocage, crkever, crkage, herever, herage, UD5ILLANY, ... (other variables)
```
Then we save the dataframe into a rds file for storage purposes and continue to the other datasets.

The cleaning for the census dataset goes 

To clean the CBSA data set, the readxl package was used to read the CBSA excel file. The data was already pretty clean, but in the original file there was a blank row and a few header rows, so we had to make sure to skip those. Then the columns were labeled and any missing values were converted to read as NA. A similar procedure was followed to read in an excel file containing Census data. With the Census data, we were interested in county populations by state. One thing we had to consider with the Census data was that it contained a row at the start of each state where the county name and the state name were the same and this row listed the state’s total population. While we did not remove this row when cleaning the data set, we were aware of it and would filter it out if necessary. For example, it was not necessary to filter out this row when we created a merged CBSA/CENSUS dataset since we joined the datasets on the conditions that a pair of state and county names matched in both datasets and since the CBSA data did not have any such pairs, it would automatically be eliminated. 

One important thing that we had to look into when cleaning the CBSA and CENSUS data sets was the column types. We tried using the character() function to tell us the types of columns we were dealing with to pass into the read_excel() function. However, we still ran into parsing errors because the excel files had numeric values stored as texts. To address this issue, we used a feature of read_excel() that allowed us to pass in a column type “guess”, allowing R to guess at/decide on how it wanted to read in the numeric values stored as text which fixed the error message and produced readable data. Once this error had been fixed, we were able to save the cleaned CBSA and Census data sets as RDS files. To support our location information data, we also read in an excel dataset containing rural urban continuum codes which was then saved as an RDS file. This data set was easy enough to clean and we employed the same techniques as those mentioned for the Census and CBSA data sets. 


----

## Rubric: On this page

You will

* Describe where/how to find data.
  * You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
  * Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
* Describe the different data files used and what each variable means. 
  * If you have many variables then only describe the most relevant ones and summarize the rest.
* Describe any cleaning you had to do for your data.
  * You *must* include a link to your `load_and_clean_data.R` file.
  * Also, describe any additional R packages you used outside of those covered in class.
  * Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
  * Some repetition of what you do in your `load_and_clean_data.R` file is fine and encouraged if it helps explain what you did.
* Organization, clarity, cleanliness of the page
  * Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.
  * This page should be self-contained.