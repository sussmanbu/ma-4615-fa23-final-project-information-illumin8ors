[
  {
    "objectID": "blog4 code.html",
    "href": "blog4 code.html",
    "title": "blog 4 :code",
    "section": "",
    "text": "load(“C:/Users/73113/Downloads/NSDUH-2021-DS0001-bndl-data-r_v3/NSDUH_2021.RData”) library(tidyverse) library(readr) drug_health_data_clean_new &lt;- PUF2021_100622 |&gt; select(QUESTID2, filedate, cigwilyr,cigtry,cigrec,CIG30AV, alctry, alcrec, aldaypwk, ALCUS30D, AL30EST, mjage, mjrec, mrdaypwk, MJDAY30A, MR30EST, cocage, cocrec, ccdaypwk, CC30EST, crkage, crdaypwk, crakrec, CR30EST, herage, herrec, hrdaypwk, HR30EST, hallucage, hallucrec, halldypwk, HALLUC30E, lsdage, pcpage, ecstmoage, inhalage, inhalrec, inhdypwk, INHAL30ES, methamage, methamrec, methdypwk, METHAM30E, oxcnanyyr, oxcnnmage, fentanyyr, pnranyrec, PNRNM30ES, PNRNM30AL, pnrwygamt, pnrwygamt, pnrwyoftn, pnrwylngr, trqanylif, trqanyrec, stmanylif, stmanyrec, sedanylif, sedanyrec, iicigrc, II2CIGRC, iralcrc, II2ALCRC, iicrkrc, II2CRKRC, alcyrtot, cocyrtot, crkyrtot, heryrtot, hallucyfq, methamyfq, ircigfm, ircgrfm, IRSMKLSS30N, iralcfm, irmjfm, ircocfm, ircrkfm, cigflag, cigyr, cigmon, cgrflag, cgryr, cgrmon, pipflag, pipmon, smklssflag, smklssyr, smklssmon, tobflag, tobyr, tobmon, AD_MDEA1,AD_MDEA2, AD_MDEA3, AD_MDEA4, yther, yshsw, YUMHTELYR2, yowrslow, yowrdcsn, YO_MDEA1, YO_MDEA2, YO_MDEA3, YO_MDEA4, YO_MDEA5, YO_MDEA6, YO_MDEA7, YO_MDEA8, yopsrels, ymdelt, cadrpeop, casurcvr, camhprob, vapanyevr, irvapanyrec, CAMHPROB2, vaptypemon, udaltimeget, udaltrystop, udalstopact, udalwdsweat, udalavwsvtr, udmjtimeget, udmjlesseff, udmjnotstop, udmjwddeprs, udcctimeget, udccwantbad, udccstrurge, udccstopact, udhetimeuse, udhenotstop,udhehlthprb, udhestopact, udhatimeuse, booked, bkdrvinf, txevrrcvd, txyrprisn, txyrslfhp, txltpyhins, snysell, snyattak, snrldcsn, yeatndyr, yehmslyr, yestscig, DSTNRV30, suiplanyr, irmedicr, irmcdchp, irchmpus, irchmpus, irprvhlt, irothhlt, irfamsoc, irfamssi, irfstamp, irfampmt, IRPINC3, IRFAMIN3, PDEN10, COUTYP4, MAIIN102, AIIND102, ENRLCOLLFT2, wrkhadjob, sexident, milstat, NEWRACE2, income, POVERTY3, PDEN10, COUTYP4) |&gt; mutate(NEWRACE2 = recode(NEWRACE2, “1” = “NonHisp White”, “2” = “NonHisp Black/Afr Am”, “3” = “NonHisp Native Am/AK Native”, “4” = “NonHisp Native HI/Other Pac Isl”, “5” = “NonHisp Asian”, “6” = “NonHisp more than one race”, “7” = “Hispanic”))\nsummary_by_race &lt;- drug_health_data_clean_new %&gt;% filter(MJDAY30A &lt;= 30) %&gt;% group_by(NEWRACE2) %&gt;% summarise( Mean_MJDAY30A = mean(MJDAY30A, na.rm = TRUE), Median_MJDAY30A = median(MJDAY30A, na.rm = TRUE), Max_MJDAY30A = max(MJDAY30A, na.rm = TRUE), Min_MJDAY30A = min(MJDAY30A, na.rm = TRUE) ) ggplot(summary_by_race, aes(x = NEWRACE2, y = Mean_MJDAY30A, fill = NEWRACE2)) + geom_bar(stat = “identity”) + labs(title = “Mean Marijuana Use in the Last 30 Days by Race”, x = “Race”, y = “Mean Days of Marijuana Use”) + theme(axis.text.x = element_text(angle = 45, hjust = 1),legend.position = “none”) + coord_cartesian(ylim = c(1, 30))\nglm_model &lt;- glm(MJDAY30A ~ NEWRACE2, data = drug_health_data_clean_new, family = poisson(link = “log”))"
  },
  {
    "objectID": "posts/2023-11-13-blog-post-4-/blog-post-4-.html",
    "href": "posts/2023-11-13-blog-post-4-/blog-post-4-.html",
    "title": "Blog Post 4",
    "section": "",
    "text": "This is a plot that shows the ages that people first used cocaine and the plot is faceted by race. On the y-axis, the frequency tells us the number of survey takers that had the same response. This tells us that people in the “NonHisp White” category had the highest number of people who had ever used cocaine, and there is a common trend across all race categories that the most typical age range when cocaine was first used was between 10 and 30 years old. This result is informative, but it also raised a concern because it made us think that the number of respondents per race category was possibly skewed and led us to question how many survey takers fit into each race category.\n\n\n\n\nThe boxplot titled “TOTAL # OF DAYS USED ALCOHOL IN PAST 12 MOS by RACE (Excluding &gt;365 days)” visually represents the distribution of the total number of days alcohol was used in the past 12 months across different racial groups. This visualization is particularly useful for understanding the central tendency, variability, and presence of outliers in the data. In detail, the Asian category shows a slightly lower median compared to others, suggesting a marginally lesser median frequency of alcohol use within this group over the period. Notably, the White category exhibits the largest box, indicating greater variability or dispersion in alcohol use days among individuals in this group. Moreover, in this plot, the Hispanic and Black categories show more outliers on the higher end of the spectrum. This suggests that there are more instances of significantly higher alcohol use days in these groups compared to the typical range observed within the groups.\n\n\n\n \nThis logistic regression model shows many things that can be broken down into understanding. First the intercept used for this model was the Hispanic population and this is due to the dataset being split into Non Hispanic and Hispanic so using the Hispanic population as a baseline was easier for analysis. The coefficient for the intercept/Hispanic population shows that when all other variables are being held constant, the log odds for Hispanic population is -0.70059. This is just in a sense the baseline to compare to the other races. For the Asian population of this dataset, when converting the log odds to odd ratio, the odds of an Asian population smoking cigarettes is 0.6482 times the odds of Hispanics smoking cigarettes. This means that the odds of the Asian population smoking cigarettes is lower than the Hispanic population. This can be seen for any values less than 1 for race categories but we see a change once we get to Non Hispanics that are more than one race where they are greater than 1 odd ratios meaning they have a higher odd ratio of smoking cigarettes compared to Hispanics. The White race group has the highest odd ratio compared to Hispanics and other groups, making them more likely to smoke cigarettes than Hispanic people. In terms of the numbers from the model, White people in the dataset have 1.9676 times the odds of smoking cigarettes compared to Hispanics.\nThis model works and fits for this dataset because the p values for each of the variables of races are lower than 0.05 showing they have significant value. The model also predicts that cigarette use and race groups have a significant relationship based on the amount of stars placed on each row, showing an emphasis on the relationship between the two. It suggests that there are differences in race groups for cigarette smoking trying it for the first time which can be explored further with data either from the outside or other variables that might have relationships with it such as income or geographic location.\nThese are some of the simple relationships we were able to notice upon EDA. Exploring these further is crucial in making key conclusions and hypothesis."
  },
  {
    "objectID": "posts/2023-11-13-blog-post-4-/blog-post-4-.html#a-look-into-the-relationships-eda",
    "href": "posts/2023-11-13-blog-post-4-/blog-post-4-.html#a-look-into-the-relationships-eda",
    "title": "Blog Post 4",
    "section": "",
    "text": "This is a plot that shows the ages that people first used cocaine and the plot is faceted by race. On the y-axis, the frequency tells us the number of survey takers that had the same response. This tells us that people in the “NonHisp White” category had the highest number of people who had ever used cocaine, and there is a common trend across all race categories that the most typical age range when cocaine was first used was between 10 and 30 years old. This result is informative, but it also raised a concern because it made us think that the number of respondents per race category was possibly skewed and led us to question how many survey takers fit into each race category.\n\n\n\n\nThe boxplot titled “TOTAL # OF DAYS USED ALCOHOL IN PAST 12 MOS by RACE (Excluding &gt;365 days)” visually represents the distribution of the total number of days alcohol was used in the past 12 months across different racial groups. This visualization is particularly useful for understanding the central tendency, variability, and presence of outliers in the data. In detail, the Asian category shows a slightly lower median compared to others, suggesting a marginally lesser median frequency of alcohol use within this group over the period. Notably, the White category exhibits the largest box, indicating greater variability or dispersion in alcohol use days among individuals in this group. Moreover, in this plot, the Hispanic and Black categories show more outliers on the higher end of the spectrum. This suggests that there are more instances of significantly higher alcohol use days in these groups compared to the typical range observed within the groups.\n\n\n\n \nThis logistic regression model shows many things that can be broken down into understanding. First the intercept used for this model was the Hispanic population and this is due to the dataset being split into Non Hispanic and Hispanic so using the Hispanic population as a baseline was easier for analysis. The coefficient for the intercept/Hispanic population shows that when all other variables are being held constant, the log odds for Hispanic population is -0.70059. This is just in a sense the baseline to compare to the other races. For the Asian population of this dataset, when converting the log odds to odd ratio, the odds of an Asian population smoking cigarettes is 0.6482 times the odds of Hispanics smoking cigarettes. This means that the odds of the Asian population smoking cigarettes is lower than the Hispanic population. This can be seen for any values less than 1 for race categories but we see a change once we get to Non Hispanics that are more than one race where they are greater than 1 odd ratios meaning they have a higher odd ratio of smoking cigarettes compared to Hispanics. The White race group has the highest odd ratio compared to Hispanics and other groups, making them more likely to smoke cigarettes than Hispanic people. In terms of the numbers from the model, White people in the dataset have 1.9676 times the odds of smoking cigarettes compared to Hispanics.\nThis model works and fits for this dataset because the p values for each of the variables of races are lower than 0.05 showing they have significant value. The model also predicts that cigarette use and race groups have a significant relationship based on the amount of stars placed on each row, showing an emphasis on the relationship between the two. It suggests that there are differences in race groups for cigarette smoking trying it for the first time which can be explored further with data either from the outside or other variables that might have relationships with it such as income or geographic location.\nThese are some of the simple relationships we were able to notice upon EDA. Exploring these further is crucial in making key conclusions and hypothesis."
  },
  {
    "objectID": "posts/2023-11-20-blog-post-5/blog-post-5.html",
    "href": "posts/2023-11-20-blog-post-5/blog-post-5.html",
    "title": "Blog Post 5",
    "section": "",
    "text": "To find appropriate data sets that would align with our data, we had to first look into how the location data was collected. The public access data file for the SAMHSA NSDUH 2021 has somewhat limited geographic information, but we have come up with some creative ways to exploit what is there. They provide a variable, “PDEN10”, with information about population density, telling us how many respondents live in a CBSA with 1 million or more people, less than 1 million people, or not in a CBSA, where CBSA stands for “Core-based statistical area”. This variable also uses 2010 census data. To expand upon this variable, we have downloaded data sets from the 2009 CBSA and the 2010 census, which match the data that the original researchers used in the survey. Our plan is to combine the census and CBSA codes, we will divide the census data set by filtering for counties with 1 million and less than 1 million inhabitants, then we will match those with CBSA codes and group/match the survey answers to those counties/CBSA codes. We will group all counties with no CBSA code into an “other” category.\nAnother variable of interest is the “COUNTYP4” column, which describes how many people are in large, small, or non-metro areas. This variable is based on 2013 Rural/Urban continuum codes. We have found that dataset and we will be using it to break down the counties that are classified as large, small, or non-metro to provide further information about which counties and states the respondents of the data set come from. These are the two most general variables concerning location information, but there are others that use the 2010 census data to describe how many people taking the survey live in an American-Indian census block or not. Thus we can also use the 2010 census data set to discover where those areas are to better inform our geographic data.\nThrough such integration, we can establish a more comprehensive, multidimensional dataset, providing a deeper background for our analysis. The selection and integration of these datasets will enable us to explore patterns of drug and substance abuse from various perspectives, gaining a more comprehensive understanding of this issue under the influence of geographical and demographic factors."
  },
  {
    "objectID": "posts/2023-11-20-blog-post-5/blog-post-5.html#understanding-the-datasets",
    "href": "posts/2023-11-20-blog-post-5/blog-post-5.html#understanding-the-datasets",
    "title": "Blog Post 5",
    "section": "",
    "text": "To find appropriate data sets that would align with our data, we had to first look into how the location data was collected. The public access data file for the SAMHSA NSDUH 2021 has somewhat limited geographic information, but we have come up with some creative ways to exploit what is there. They provide a variable, “PDEN10”, with information about population density, telling us how many respondents live in a CBSA with 1 million or more people, less than 1 million people, or not in a CBSA, where CBSA stands for “Core-based statistical area”. This variable also uses 2010 census data. To expand upon this variable, we have downloaded data sets from the 2009 CBSA and the 2010 census, which match the data that the original researchers used in the survey. Our plan is to combine the census and CBSA codes, we will divide the census data set by filtering for counties with 1 million and less than 1 million inhabitants, then we will match those with CBSA codes and group/match the survey answers to those counties/CBSA codes. We will group all counties with no CBSA code into an “other” category.\nAnother variable of interest is the “COUNTYP4” column, which describes how many people are in large, small, or non-metro areas. This variable is based on 2013 Rural/Urban continuum codes. We have found that dataset and we will be using it to break down the counties that are classified as large, small, or non-metro to provide further information about which counties and states the respondents of the data set come from. These are the two most general variables concerning location information, but there are others that use the 2010 census data to describe how many people taking the survey live in an American-Indian census block or not. Thus we can also use the 2010 census data set to discover where those areas are to better inform our geographic data.\nThrough such integration, we can establish a more comprehensive, multidimensional dataset, providing a deeper background for our analysis. The selection and integration of these datasets will enable us to explore patterns of drug and substance abuse from various perspectives, gaining a more comprehensive understanding of this issue under the influence of geographical and demographic factors."
  },
  {
    "objectID": "posts/2023-11-20-blog-post-5/blog-post-5.html#process-and-next-steps",
    "href": "posts/2023-11-20-blog-post-5/blog-post-5.html#process-and-next-steps",
    "title": "Blog Post 5",
    "section": "Process and Next Steps",
    "text": "Process and Next Steps\nWe did not combine the data yet as we are viewing it first individually and then we will combine the data. We do not expect any difficulties with combining it because we can use the RUCC and the CBSA with the census data to get a better understanding of the metro data from the NSDUH data. The process to join the data is to first take the RUCC and associate it with the names of counties in the United States. Then using the CBSA and the census data, we can associate the CBSA with an area in the United States, giving us access to census population which is correlated to the NSDUH calculation of population metric for respondents and where they can be from. This gives us a deeper understanding of where respondents are most likely to be from based off of CBSA and Census data and whether they are from large or small cities based on RUCC as well.\nOur next steps for these data sets would be to combine and then create relationships between drug use and certain large metros that can be correlated with the population of such top metros. This data gives us the best approximation of where respondents may be from and also gives correlations for how these large or small metros and also CBSA codes with the counties census can be seen with drug related problems. We want to take our analysis even further and hopefully use some models that can help give a deeper understanding of what is going on with such respondent info. We also want to explore more relationships between certain variables in the NSDUH dataset and see if we can use such relationships to go even deeper into how location plays a role in affecting such relationships. Exploring this further is crucial in understanding drug and substance abuse."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post.\nEvery time you want to make a new post, you can repeat step 2 above. When you want to publish your progress, follow steps 4-7 from Customize your site.\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2023-10-30-blog-post-2/blog-post-2.html",
    "href": "posts/2023-10-30-blog-post-2/blog-post-2.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "The principles emphasize the importance of considering the impact of data collection, analysis, and dissemination on the communities and individuals being studied. While our data analysis may not directly involve community interaction, we can apply these principles by ensuring that the data we use are collected and analyzed in a way that respects the principles of beneficence, respect for persons, and justice. This includes considering the potential benefits and harms of the data, being transparent about the data’s limitations, and striving for a fair distribution of burdens and benefits.\nSince we are not the ones actually collecting the data, we cannot really consider the principles in relation to that beyond choosing data sets from reputable and ethical sources, but we can think about the principles in their relation to data cleaning, visualization, and analysis. For example, we want to make sure we ask interesting but respectful/considerate questions for our analysis. We also want to make sure, while cleaning the data, that we do eliminate data that is relevant to the groups we are looking at, we do not want to be biased in cleaning our data because we want to keep our analysis representative of the survey participants. Adding on, being able to fairly assess and understand what the data is representing is key to not misinterpret and fall short in analysis. Transparency is key for the audience to understand how the data is being collected and shown so that misinterpretations can be minimized to the best possible degree.\nThe principles of equitable data practice discussed in the article are: beneficence, justice, and respect for persons. When looking at our data set, we have to make sure that there is data available for a diverse range of different demographics. This idea ties into the justice principle because we want to make sure that our data represents multiple perspectives and communities. While our data includes data on income, age, health insurance, and race, it lacks detailed information on the location of the respondents. It would be good to supplement our data with another data set that gives us information about the general location demographics of the participants. Then, beneficence comes in because we want to be aware how sensitive topics can affect certain groups. Since our data concerns substance abuse and mental health disorders, we have to be sensitive when looking at the data and with the questions we decide to ask about it.\nAdhering to the practice of transparency would entail providing clear and comprehensive information about the data, including its limitations and potential sources of bias. It would involve documenting the data collection and analysis processes, making sure the data’s context is well-understood, and acknowledging any potential misinterpretations or misuses of the data. Additionally, the principle of respect for persons would require obtaining informed consent when necessary and protecting the privacy of individuals whose data is used. This helps in maintaining respect amongst individuals for their data collected and also having privacy concerns lowered so that individuals do not fear their data is being used for the wrong purposes.\nIn our analysis, it is possible that due to limited sample size, we were unable to collect a certain amount of data for each race. To address this, we could filter the data set to make sure we are analyzing a comparable amount of data for each race category to make sure we are not biasing the data or producing false data analysis. At the same time, the data itself may have limitations due to the fact that the respondents who can be contacted have similar identities to the investigators. In terms of potential abuse or misuse, if the data is not adequately protected, it could be accessed or used in ways that harm individuals or communities.\nThere might be sampling bias. If the data is not representative of the entire population, conclusions drawn from it might not be generalizable. For instance, certain demographics might be underrepresented or over represented. Moreover, data related to substance use often relies on self-reporting, which can be subject to recall bias and social desirability bias. If not handled carefully, data on substance use can contribute to the stigmatization of individuals who use or have used substances, potentially leading to discrimination or other negative outcomes. Furthermore, Without proper context or understanding, the data can be misinterpreted. For example, high rates of substance use in a particular demographic might be wrongly attributed to cultural or inherent factors rather than socio-economic or environmental reasons. This can further cause divides and lead back to the topic of misinterpretation which hinder from the point that we as a group are trying to convey in our reports and plots."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (Next week, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext week, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-11-06-blog-post-3/blog-post-3.html",
    "href": "posts/2023-11-06-blog-post-3/blog-post-3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "To approach the data in a way such that we can do efficient cleaning but also have the columns and rows we need to accurately assess the data, we first needed to understand what was recorded from the dataset. Once we understood that we divided and conquered the codebook associated with this data from the SAMHSA. We needed to look at questions answered in the survey such that they had a high response rate and also that they would be able to help in further analysis as well. We are focusing more on cutting many unnecessary columns that would not be as used throughout but also columns that tend to have high no response rates as well. This strategy is effective for the beginning as it gives a very good sense with what we are working with.\nSome of the important columns we needed were which types of drugs people had tried and certain questions like if they have used in the past 30 days. We were also interested in the columns that reported on the age when a person used a substance because we could make an interesting comparison between ages a substance was first used by race. Then we also made sure to include questions about mental health and whether each respondent went out to receive treatment for mental health. This data could allow us to look at relationships between race, mental health treatment, and drug use. Other columns that were important are ones that discuss location as this will relate what we need if we need other datasets to incorporate into it with more precise location . Relating mental health with location as well as drug use to location can help with economic datasets per location be used as well as a follow up to further support the dataset and create more analysis. Another interesting comparison would be to look at the relationship between age of first use of a substance by race to see if there is a good trend/comparison there.\nNot only was this the thinking of our group for the data cleaning strategy, we also quickly realized how important it is to look at certain outliers and data points that might not work for analysis. One of the important points mentioned before was high non response rates which we quickly saw through in the table that we filtered for just cigarettes to understand how much is really missing. The codebook also helps in understanding this as well. This can be quickly seen through a look at the table with all the columns pictured below in the table sample: \nQuickly looking into just this little sample shows that the cigyfu column has a bunch of skipped answers based on the codebook notation as well to support this. The 9999 and 99 and 991 are all variations of “NA” or did not want to respond to the survey to such a particular question as indicated by the codebook. As we continue to go through the data set we will be cautious to check for outliers."
  },
  {
    "objectID": "posts/2023-10-23-blog-post-1/blog-post-1.html",
    "href": "posts/2023-10-23-blog-post-1/blog-post-1.html",
    "title": "Blog Post 1 - Datasets Search",
    "section": "",
    "text": "We were very invested in taking the route of exploring racial disparity with drug use, income and mental illness and felt these datasets below represent the topic we are interested in very well.\nThe first dataset that we had found was within this link: “https://data.world/balexturner/drug-use-employment-work-absence-income-race-education”.This dataset is called “NSDUH Workforce Adults”. This dataset has 64 columns and 32,040 rows and is taken from the health survey conducted in 2015 by the Substance Abuse and Mental Health Services Administration. Since this is a subset selected it is supposed to be randomly selected for optimal analysis. Loading the data was easy as it was a csv easily loaded into R Studio. We have not started cleaning the dataset but we are able to parse through the dataset and get specific columns and data points we might need making it easier for us. The ease of loading it into R Studio and the ability to extract specific columns and data points make this dataset a promising resource for conducting a comprehensive exploration of various socio-economic and health-related factors among adult populations\nThe second dataset that we had found was within this link “https://www.icpsr.umich.edu/web/NACJD/studies/27521/variables”. The dataset is called “Gender, Mental Illness, and Crime in the United States, 2004”. This dataset has 3011 columns and for rows it has more than 2GB of data and the last row is 55,602. The data was collected from a 2004 survey on drug use and health to the public. The data is plentiful with data but however is not up to date with current date and the surveys were major revamped. There are columns that were added to this dataset by the researchers of this dataset. We were able to load the data and get the columns and be able to see some of the sample responses. The only problem with this dataset would be that it may be too old for use but it does contain information that would be useful for relating racial disparity, drug use and justice system.\nThe third dataset that we had found is “https://www.datafiles.samhsa.gov/dataset/national-survey-drug-use-and-health-2021-nsduh-2021-ds0001”. This dataset is straight from the SAMHSA (Substance Abuse and Mental Services Administration) which conducts surveys directly and collects and asks questions. There is an associated codebook for the data that can help us determine relationships between certain drugs and income and much more as well. Some of the collection methods were web-interviewing and more and cannot be compared at all to previous years according to the website, which offers the most up to date collection. This NSDUH dataset has 2,989 columns and 58,034 entries of rows. It has very big general population coverage in order to analyze. We were able to load the R data into R studio and convert it into a csv as well. Being able to parse the data was also fairly simple which helps in finding relationships between race and medical conditions and more. This is the dataset we are most excited for because of how fairly new it is and how many ways we can go about this topic with this many variables/questions.\nSince we have found three very expansive and diverse data sets, we know that there is a lot of potential for questions we can explore. In our first data set, from the NDSUH codebook linked above, there are several columns of interest to us. This data set covers race, income, and location in relation to drug and alcohol use. Thus we can ask questions such as: what is the link between race and lifetime drug or alcohol use? If we filter by lower income areas, is the link between race and drug/alcohol use stronger? Is income or race a more important influencing factor in drug/alcohol use? These questions can be addressed by the data we have found and we are excited to learn more about how to visualize the data to best address trends in our data. The data set from data.world is also a great option and it has several variables of interest to us. It provides links between drug use, employment rates, work absence, income, race, and education. This would allow us to ask questions such as: Is education level or employment rate by race a stronger influencing factor towards drug use? What is the variance of income by race? What is the average drug use by race and income level and which seems to have more of an influence on drug use? While this data set is interesting and useful, it does not allow us to ask complex questions such as what kind of drugs are more heavily used by different race categories and by different income levels, as the NDSUH codebook data set would allow us to do. The data set from UMich could also be good to use, it has interesting variables such as how many days per week or month people use/buy different drugs and it would allow us to sort the data by race and gender. It is a large data set like the NDSUH one. This data set also includes information on education, income, location, and employment. It would allow us to ask similarly interesting questions.\nWe are very interested in the NDSUH codebook data set, and while it is good that the NDSUH codebook data set is so diverse, this means it is also very large. The size of it has made it hard to work with and load into our repository and this will also make it harder to clean and organize. With the UMich data set, we may struggle to use it because it is not readily available as a CSV file and we would have to convert it into a format we could work with. Because there is so much data in the NDSUH data set, with so many different ways to interpret it and questions to ask, we could potentially struggle to come up with a good question(s) that are easily addressed by the data and not get lost in all of the data we have to work with. We could accidentally choose a question that is too specific and does not capitalize on the breadth of the data set, or we could go too broad and end up with a report that is too complex and does not achieve a meaningful or understandable interpretation of the data. We will also have to work hard to choose what to filter out and how to do that because it is impossible to use the entire data set and we will have to be careful in choosing which parts of it to work with and focus on. It could also be a good idea, with any data set that we choose, to weigh drug use against different factors such as education/income/location when possible so that we can see how much race actually influences drug use so we do not accidentally try to interpret false trends."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Blog Post 5\n\n\n\n\n\n\n\nData Combination\n\n\n\n\nExploring other datasets as compliments to the original dataset.\n\n\n\n\n\n\nNov 20, 2023\n\n\nInformation Illumin8ors\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 4\n\n\n\n\n\n\n\nEDA\n\n\n\n\nTalking about EDA and some notable relationships\n\n\n\n\n\n\nNov 13, 2023\n\n\nInformation Illumin8ors\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 3\n\n\n\n\n\n\n\nData Cleaning\n\n\n\n\nGoing into the data cleaning process for the dataset used\n\n\n\n\n\n\nNov 6, 2023\n\n\nInformation Illumin8ors\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 2\n\n\n\n\n\n\n\nDataset Ethics and Equity\n\n\n\n\nData for Equity Understanding for our Data Set\n\n\n\n\n\n\nOct 30, 2023\n\n\nInformation Illumin8ors\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 1 - Datasets Search\n\n\n\n\n\n\n\nDataset Start\n\n\n\n\nDatasets Reasearch for beginning the Final Project\n\n\n\n\n\n\nOct 23, 2023\n\n\nInformation Illumin8ors\n\n\n\n\n\n\n  \n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post.\n\n\n\n\n\n\nOct 15, 2023\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting.\n\n\n\n\n\n\nOct 13, 2023\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMNAME. The members of this team are below."
  },
  {
    "objectID": "about.html#arianit-balidemaj",
    "href": "about.html#arianit-balidemaj",
    "title": "About",
    "section": "Arianit Balidemaj",
    "text": "Arianit Balidemaj\nArianit is a undergraduate student at BU. Link to github https://github.com/ArianitBalidemaj"
  },
  {
    "objectID": "about.html#eleanor-paul",
    "href": "about.html#eleanor-paul",
    "title": "About",
    "section": "Eleanor Paul",
    "text": "Eleanor Paul\nEleanor is a senior in the math department at BU. Link to github: https://github.com/eleanorpaul05"
  },
  {
    "objectID": "about.html#wenting-chen",
    "href": "about.html#wenting-chen",
    "title": "About",
    "section": "Wenting Chen",
    "text": "Wenting Chen\nWenting is a statistics students at BU. Link to github: https://github.com/coriandercwt\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you hope to ask, illustrations relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nprint(getwd())\n\n[1] \"/Users/arianith.balidemaj/Desktop/MA415/Final\"\n\ndata &lt;- read_csv(here::here(\"dataset/loan_refusal_clean.csv\"))\n\nRows: 20 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): bank\ndbl (4): min, white, himin, hiwhite\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nload(here::here(\"dataset/loan_refusal.RData\"))\nprint(ls())\n\n[1] \"data\"            \"has_annotations\" \"loan_data_clean\""
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(dplyr)\n\nfile_path &lt;- \"dataset/cleaned_data_new.rds\"\n\ncleaned_data &lt;- readRDS(file_path)\n\n# Filter out records where alcyrtot is greater than 365\nfiltered_data &lt;- cleaned_data %&gt;% \n  filter(alcyrtot &lt;= 365)\n\nboxplot_alcohol_use &lt;- ggplot(filtered_data, aes(x = factor(NEWRACE2), y = alcyrtot, fill = factor(NEWRACE2))) +\n  geom_boxplot() +\n  labs(title = \"TOTAL # OF DAYS USED ALCOHOL IN PAST 12 MOS by RACE (Excluding &gt;365 days)\",\n       x = \"Race\",\n       y = \"Total Days of Alcohol Use\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\nprint(boxplot_alcohol_use)\n\n\n\n\n\nTo be or not to be.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nprint(getwd())\n\n[1] \"/Users/arianith.balidemaj/Desktop/MA415/Final\"\n\ncleaned_data &lt;- readRDS(here::here(\"dataset/cleaned_data_new.rds\"))\n# print(ls())\n:::"
  },
  {
    "objectID": "analysis.html#logisitic-regression-on-cigeratte-use-and-race",
    "href": "analysis.html#logisitic-regression-on-cigeratte-use-and-race",
    "title": "Analysis",
    "section": "Logisitic Regression on Cigeratte Use and Race",
    "text": "Logisitic Regression on Cigeratte Use and Race\n\n# Filtering the model such that we have binary values\nlog_mod &lt;- cleaned_data |&gt;\n  select(NEWRACE2, cigtry, cigrec) |&gt;\n  filter(cigtry &lt; 984 | cigtry == 991) |&gt;\n  mutate(cigtry = ifelse(cigtry == 991, 0, cigtry)) |&gt;\n  mutate(cigrec = ifelse(cigrec == 91, 0, cigrec)) |&gt;\n  mutate(cigtry_binary = ifelse(cigtry &gt; 0, 1, 0))\n\n# Creating Logistic Regression model\nmodel &lt;- glm(cigtry_binary ~ NEWRACE2, data = log_mod, family = \"binomial\")\n\nodds_ratios &lt;- exp(coef(model))\n\nsummary(model)\n\n\nCall:\nglm(formula = cigtry_binary ~ NEWRACE2, family = \"binomial\", \n    data = log_mod)\n\nCoefficients:\n                                        Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                             -0.70059    0.02136 -32.801  &lt; 2e-16\nNEWRACE2NonHisp Asian                   -0.43348    0.04626  -9.371  &lt; 2e-16\nNEWRACE2NonHisp Black/Afr Am            -0.14870    0.03414  -4.355 1.33e-05\nNEWRACE2NonHisp more than one race       0.38987    0.04569   8.532  &lt; 2e-16\nNEWRACE2NonHisp Native Am/AK Native      0.87343    0.08605  10.150  &lt; 2e-16\nNEWRACE2NonHisp Native HI/Other Pac Isl  0.27657    0.13800   2.004   0.0451\nNEWRACE2NonHisp White                    0.67614    0.02391  28.283  &lt; 2e-16\n                                           \n(Intercept)                             ***\nNEWRACE2NonHisp Asian                   ***\nNEWRACE2NonHisp Black/Afr Am            ***\nNEWRACE2NonHisp more than one race      ***\nNEWRACE2NonHisp Native Am/AK Native     ***\nNEWRACE2NonHisp Native HI/Other Pac Isl *  \nNEWRACE2NonHisp White                   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 78932  on 57837  degrees of freedom\nResidual deviance: 76957  on 57831  degrees of freedom\nAIC: 76971\n\nNumber of Fisher Scoring iterations: 4\n\nodds_ratios_df &lt;- data.frame(\n  Race_Category = ifelse(names(odds_ratios) == \"(Intercept)\", \"Hispanic\",\n                     ifelse(names(odds_ratios) == \"NEWRACE2NonHisp Asian\", \"Asian\",\n                     ifelse(names(odds_ratios) == \"NEWRACE2NonHisp Black/Afr Am\", \"Black/Afr Am\",\n                     ifelse(names(odds_ratios) == \"NEWRACE2NonHisp more than one race\", \"NonHisp more than one race\",\n                     ifelse(names(odds_ratios) == \"NEWRACE2NonHisp Native Am/AK Native\", \"Native Am/AK Native\",\n                     ifelse(names(odds_ratios) == \"NEWRACE2NonHisp Native HI/Other Pac Isl\", \"Native HI/Other Pac Isl\",\n                     ifelse(names(odds_ratios) == \"NEWRACE2NonHisp White\", \"White\", names(odds_ratios)))))))),\n  Odds_Ratio = odds_ratios\n)\n\nodds_ratios_df\n\n                                                     Race_Category Odds_Ratio\n(Intercept)                                               Hispanic  0.4962929\nNEWRACE2NonHisp Asian                                        Asian  0.6482488\nNEWRACE2NonHisp Black/Afr Am                          Black/Afr Am  0.8618319\nNEWRACE2NonHisp more than one race      NonHisp more than one race  1.4767876\nNEWRACE2NonHisp Native Am/AK Native            Native Am/AK Native  2.3951162\nNEWRACE2NonHisp Native HI/Other Pac Isl    Native HI/Other Pac Isl  1.3185998\nNEWRACE2NonHisp White                                        White  1.9662812\n\n\nThe coefficient for the intercept/Hispanic population shows that when all other variables are being held constant, the log odds for Hispanic population is  -0.70059. This is just in a sense the baseline to compare to the other races. For the Asian population of this dataset, when converting the log odds to odd ratio, the odds of an Asian population smoking cigarettes is 0.6482 times the odds of Hispanics smoking cigarettes. This means that the odds of the Asian population smoking cigarettes is lower than the Hispanic population. In terms of the numbers from the model, White people in the dataset have 1.9676 times the odds of smoking cigarettes compared to Hispanics."
  },
  {
    "objectID": "analysis.html#note-on-attribution-1",
    "href": "analysis.html#note-on-attribution-1",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this"
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the techniques you used for validating your results.\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, use clean easy-to-read code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Importantly, these should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nInteractive\nYou will also be required to make an interactive dashboard like this one.\nYour Big Data page should include a link to an interactive dashboard. The dashboard should be created either using Shiny or FlexDashboard (or another tool with professor’s approval). This interactive component should in some way support your thesis from your big picture page. Good interactives often provide both high-level understanding of the data while allowing a user to investigate specific scenarios, observations, subgroups, etc.\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions from the Big Picture? Plotly with default hover text will get no credit. Be creative!\n\n\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset_ignore folder. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Add the files just like you would any other file.\nIf you create a folder named data this will cause problems.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour load_and_clean_data.R file is how you will load and clean your data. Here is a an example of a very simple one.\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\).\nYou might consider using the here function fomr the here package to avoid path problems.\n\n\nLoad and clean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or .RData files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/load_and_clean_data.R) which appears as cleaning script.\n\nsource(\n  \"scripts/load_and_clean_data.R\",\n  echo = TRUE # Use echo=FALSE or omit it to avoid code output  \n)\n\n\n&gt; library(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n&gt; library(readr)\n\n&gt; load(\"./dataset/NSDUH_2021.RData\")\n\n&gt; drug_health_data_clean_new &lt;- mutate(select(PUF2021_100622, \n+     QUESTID2, filedate, cigwilyr, cigtry, cigrec, CIG30AV, alctry, \n+     alcrec, ald .... [TRUNCATED] \n\n&gt; saveRDS(drug_health_data_clean_new, \"./dataset/cleaned_data_new.rds\")\n\n\n\n\nWhere to find Dataset\nWe found the data set by googling keywords such as “mental health, race, age, substance abuse” accompanied with “data set, csv, R”. This allowed us to find the codebook from the “Substance Abuse and Mental Health Services Administration”, describing a survey on substance abuse, mental health, and race from 2021. This survey was the “National Survey on Drug Use and Health”. We deemed this source reputable and we found an R file to download from: https://www.datafiles.samhsa.gov/dataset/national-survey-drug-use-and-health-2021-nsduh-2021-ds0001. The link to the codebook is here:\nhttps://www.datafiles.samhsa.gov/sites/default/files/field-uploads-protected/studies/NSDUH-2021/NSDUH-2021-datasets/NSDUH-2021-DS0001/NSDUH-2021-DS0001-info/NSDUH-2021-DS0001-info-codebook.pdf\n\n\nHow was the data collected\nThe primary purpose of this data and the NSDUH organization was to measure the frequency and relationship between substance abuse and mental health issues. The target population is the civilian and noninstitutionalized one. This survey was sent out to and filled out by individual persons in the USA, aged 12 and over, the data was collected cross-sectionally. The survey collectors warn that this data is dependent on respondent’s truthfulness and memory, and should thus be taken with a grain of salt. Also, this survey does not include data from citizens who are active-duty military members or living in institutional group shelters (homeless shelters, hospitals, prisons, etc.). They warn that this may underestimate the use of certain drugs like heroin.\n\n\nDifferent data files used\nWhile we used the R file available to download to make the cleaned data set in our final project repository, we used the codebook to actually understand the data set csv file. The codebook contained information about each column and it was broken down into sections of self-administered substance abuse, imputed substance abuse, other self-administered sections, demographics, and geographic. We divided the sections and went through them, when sorting through the substance abuse sections, we stayed consistent in the types of columns we chose, those were: “age when first used”, “time since last used”, “number of days per week used in last 12 months”, and “best estimate of how many times used in last 30 days”. ### Cleaning for the data\nThis is the link to the script, cleaning script. The Script cleans the data by keeping only the columns that have enough responses to the questions and also are highly relevant questions that can be used in understanding the data. As explained in our blog post, our strategy is too use the columns with the most value in analysis."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  }
]